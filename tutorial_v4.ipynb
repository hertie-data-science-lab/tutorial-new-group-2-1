{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "name": "tutorial_v3.ipynb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hertie-data-science-lab/tutorial-new-group-2-1/blob/xiaohan-modeling/tutorial_v4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial: Transfer Learning for Flood Mapping Using Sentinel-1 Radar Imagery\n",
        "\n",
        "\n",
        "# GRAD-E1394 Deep Learning - Assignment 3\n",
        "\n",
        "Authors:\n",
        "\n",
        "\n",
        "*   Aditi Joshi\n",
        "*   Elena Murray\n",
        "*   Leticia Figueiredo Collado\n",
        "*   Sattiki Ganguly\n",
        "*   Xiaohan Wu\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BPrPCBoKoVny"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test - check commit."
      ],
      "metadata": {
        "id": "seIIHEFETwi7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install segmentation-models-pytorch --quiet\n",
        "!pip install pretrainedmodels --quiet\n",
        "!pip install efficientnet-pytorch --quiet"
      ],
      "metadata": {
        "id": "4GHQh3OEueFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil ls gs://sen1floods11/v1.1/catalog/sen1floods11_hand_labeled_label/ > chip_list.txt\n",
        "\n",
        "with open(\"chip_list.txt\") as f:\n",
        "    chip_dirs = [line.strip() for line in f]\n",
        "\n",
        "chip_dirs = [d for d in chip_dirs if d.endswith(\"_label/\")]\n",
        "\n",
        "def parse_chip(d):\n",
        "    name = d.rstrip(\"/\").split(\"/\")[-1].replace(\"_label\", \"\")\n",
        "    country = name.split(\"_\")[0]\n",
        "    return country, name\n",
        "\n",
        "chip_info = [parse_chip(d) for d in chip_dirs]"
      ],
      "metadata": {
        "id": "N9pPym85uVKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count chips per country\n",
        "from collections import Counter\n",
        "country_counts = Counter([country for country, _ in chip_info])\n",
        "\n",
        "print(\"Chips per country:\")\n",
        "for country, count in sorted(country_counts.items(), key=lambda x: -x[1])[:10]:\n",
        "    print(f\"  {country}: {count}\")"
      ],
      "metadata": {
        "id": "yHqCyhnj60dl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_per_country = 50\n",
        "selected_countries = [\"India\", \"USA\", \"Paraguay\"]\n",
        "\n",
        "selected_chips = []\n",
        "for country in selected_countries:\n",
        "    country_chips = [name for c, name in chip_info if c == country]\n",
        "    # Take up to target_per_country from each\n",
        "    selected_chips.extend(country_chips[:target_per_country])\n",
        "\n",
        "print(f\"\\nSelected {len(selected_chips)} chips from {len(selected_countries)} countries\")\n",
        "print(f\"Target: ~150 chips for balanced dataset\")"
      ],
      "metadata": {
        "id": "rq2OK1Um64FV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import rasterio\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import torchvision.transforms.functional as F\n",
        "import torchvision.transforms as T\n",
        "import random\n",
        "\n",
        "# GCS streaming prefixes\n",
        "HTTP_PREFIX = \"https://storage.googleapis.com/sen1floods11/v1.1\"\n",
        "S1_PREFIX    = f\"/vsicurl/{HTTP_PREFIX}/data/flood_events/HandLabeled/S1Hand\"\n",
        "LABEL_PREFIX = f\"/vsicurl/{HTTP_PREFIX}/data/flood_events/HandLabeled/LabelHand\"\n",
        "\n",
        "class Sentinel1FloodDataset(Dataset):\n",
        "    def __init__(self, id_list, augment = False):\n",
        "        self.ids = id_list\n",
        "        self.augment = augment\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        cid = self.ids[idx]\n",
        "\n",
        "        s1_path    = f\"{S1_PREFIX}/{cid}_S1Hand.tif\"\n",
        "        label_path = f\"{LABEL_PREFIX}/{cid}_LabelHand.tif\"\n",
        "\n",
        "        # --- Load Sentinel-1 SAR image (VV/VH) ---\n",
        "        with rasterio.open(s1_path) as src:\n",
        "            s1_img = src.read().astype(\"float32\")  # (2, 512, 512)\n",
        "\n",
        "        # Robust SAR normalization\n",
        "        s1_img = np.nan_to_num(s1_img)\n",
        "        s1_img = np.clip(s1_img, -50, 50)\n",
        "        s1_img = np.log1p(s1_img - s1_img.min())\n",
        "        s1_img = (s1_img - s1_img.mean()) / (s1_img.std() + 1e-6)\n",
        "\n",
        "        # --- Load flood mask ---\n",
        "        with rasterio.open(label_path) as src:\n",
        "            mask_raw = src.read(1).astype(\"int16\")\n",
        "\n",
        "        valid_mask = (mask_raw != -1)\n",
        "        label = (mask_raw == 1).astype(\"float32\")\n",
        "\n",
        "        # Data augmentation\n",
        "        if self.augment:\n",
        "            if random.random() > 0.5:\n",
        "                s1_img = np.flip(s1_img, axis=2).copy()\n",
        "                label = np.flip(label, axis=1).copy()\n",
        "                valid_mask = np.flip(valid_mask, axis=1).copy()\n",
        "\n",
        "            if random.random() > 0.5:\n",
        "                s1_img = np.flip(s1_img, axis=1).copy()\n",
        "                label = np.flip(label, axis=0).copy()\n",
        "                valid_mask = np.flip(valid_mask, axis=0).copy()\n",
        "\n",
        "            k = random.randint(0, 3)\n",
        "            if k > 0:\n",
        "                s1_img = np.rot90(s1_img, k, axes=(1, 2)).copy()\n",
        "                label = np.rot90(label, k, axes=(0, 1)).copy()\n",
        "                valid_mask = np.rot90(valid_mask, k, axes=(0, 1)).copy()\n",
        "\n",
        "        x = torch.tensor(s1_img, dtype=torch.float32)\n",
        "        y = torch.tensor(label, dtype=torch.float32)[None, ...]\n",
        "        valid = torch.tensor(valid_mask, dtype=torch.bool)[None, ...]\n",
        "\n",
        "        return x, y, valid"
      ],
      "metadata": {
        "id": "JKBD4lzxtZzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tensor_to_pil_pair(x, y, valid):\n",
        "    vv = x[0].cpu().numpy()\n",
        "    vh = x[1].cpu().numpy()\n",
        "    label_arr = y[0].cpu().numpy()\n",
        "    valid_arr = valid[0].cpu().numpy().astype(np.uint8)\n",
        "\n",
        "    # Normalize to 0-255 range for visualization\n",
        "    vv_norm = ((vv - vv.min()) / (vv.max() - vv.min() + 1e-8) * 255).astype(np.uint8)\n",
        "    vh_norm = ((vh - vh.min()) / (vh.max() - vh.min() + 1e-8) * 255).astype(np.uint8)\n",
        "\n",
        "    vv_pil = Image.fromarray(vv_norm)\n",
        "    vh_pil = Image.fromarray(vh_norm)\n",
        "    label_pil = Image.fromarray((label_arr * 255).astype(np.uint8))\n",
        "    valid_pil = Image.fromarray((valid_arr * 255).astype(np.uint8))\n",
        "\n",
        "    return vv_pil, vh_pil, label_pil, valid_pil"
      ],
      "metadata": {
        "id": "9qdNC7GjthkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_test(x, y, valid):\n",
        "    return x, y, valid"
      ],
      "metadata": {
        "id": "jDarpVMbt6Nw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_ids = sorted(selected_chips)\n",
        "\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(valid_ids)\n",
        "\n",
        "n = len(valid_ids)\n",
        "train_ids = valid_ids[:int(0.7*n)]\n",
        "val_ids   = valid_ids[int(0.7*n):int(0.85*n)]\n",
        "test_ids  = valid_ids[int(0.85*n):]\n",
        "\n",
        "print(f\"Train: {len(train_ids)}  Val: {len(val_ids)}  Test: {len(test_ids)}\")"
      ],
      "metadata": {
        "id": "nwGhksuguPsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 4\n",
        "\n",
        "train_ds = Sentinel1FloodDataset(train_ids, augment=True)\n",
        "val_ds   = Sentinel1FloodDataset(val_ids,   augment=False)\n",
        "test_ds  = Sentinel1FloodDataset(test_ids,  augment=False)\n",
        "\n",
        "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_dl   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)\n",
        "test_dl  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "fm52Yv_Nt_1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import segmentation_models_pytorch as smp\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# U-Net with ResNet34 encoder pre-trained on ImageNet  ← TRANSFER LEARNING\n",
        "model = smp.Unet(\n",
        "    encoder_name=\"resnet34\",\n",
        "    encoder_weights=\"imagenet\",   # this is the transfer part\n",
        "    in_channels=2,                # VV + VH\n",
        "    classes=1                     # binary mask\n",
        ").to(device)\n",
        "\n",
        "# Binary cross entropy with logits loss\n",
        "# Combines sigmoid activation + BCE loss for numerical stability\n",
        "criterion = nn.BCEWithLogitsLoss()"
      ],
      "metadata": {
        "id": "OpglqQeQuale"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_iou_from_logits(logits, target, valid):\n",
        "    \"\"\"\n",
        "    Compute Intersection over Union (IoU) for flood segmentation.\n",
        "\n",
        "    Args:\n",
        "        logits: (B,1,H,W) - raw model outputs (before sigmoid)\n",
        "        target: (B,1,H,W) - ground truth labels (0/1)\n",
        "        valid:  (B,1,H,W) - validity mask (True for valid pixels)\n",
        "\n",
        "    Returns:\n",
        "        iou: scalar tensor - IoU score\n",
        "    \"\"\"\n",
        "    probs = torch.sigmoid(logits)\n",
        "    preds = (probs > 0.5).float()\n",
        "\n",
        "    v = valid.bool()\n",
        "    if v.sum() == 0:\n",
        "        return torch.tensor(0.0, device=logits.device)\n",
        "\n",
        "    p = preds[v]\n",
        "    t = target[v]\n",
        "\n",
        "    intersection = (p * t).sum()\n",
        "    union = p.sum() + t.sum() - intersection\n",
        "    iou = (intersection + 1e-6) / (union + 1e-6)\n",
        "    return iou\n",
        "\n",
        "\n",
        "def compute_accuracy_from_logits(logits, target, valid):\n",
        "    \"\"\"\n",
        "    Compute pixel-wise accuracy for flood segmentation.\n",
        "\n",
        "    Args:\n",
        "        logits: (B,1,H,W) - raw model outputs (before sigmoid)\n",
        "        target: (B,1,H,W) - ground truth labels (0/1)\n",
        "        valid:  (B,1,H,W) - validity mask (True for valid pixels)\n",
        "\n",
        "    Returns:\n",
        "        acc: scalar tensor - accuracy score\n",
        "    \"\"\"\n",
        "    probs = torch.sigmoid(logits)\n",
        "    preds = (probs > 0.5).float()\n",
        "\n",
        "    v = valid.bool()\n",
        "    if v.sum() == 0:\n",
        "        return torch.tensor(0.0, device=logits.device)\n",
        "\n",
        "    p = preds[v]\n",
        "    t = target[v]\n",
        "\n",
        "    correct = (p == t).float().sum()\n",
        "    acc = correct / p.numel()\n",
        "    return acc"
      ],
      "metadata": {
        "id": "Q1lEehOvuphM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, dl, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_iou = 0.0\n",
        "    total_acc = 0.0\n",
        "    n_batches = 0\n",
        "\n",
        "    for x, y, valid in dl:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        valid = valid.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(x)  # (B,1,H,W)\n",
        "\n",
        "        if valid.sum() == 0:\n",
        "            continue\n",
        "\n",
        "        loss = criterion(logits[valid], y[valid])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        iou = compute_iou_from_logits(logits, y, valid).item()\n",
        "        acc = compute_accuracy_from_logits(logits, y, valid).item()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_iou  += iou\n",
        "        total_acc  += acc\n",
        "        n_batches  += 1\n",
        "\n",
        "    if n_batches == 0:\n",
        "        return 0.0, 0.0, 0.0\n",
        "\n",
        "    return (\n",
        "        total_loss / n_batches,\n",
        "        total_iou  / n_batches,\n",
        "        total_acc  / n_batches,\n",
        "    )\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate_one_epoch(model, dl):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_iou = 0.0\n",
        "    total_acc = 0.0\n",
        "    n_batches = 0\n",
        "\n",
        "    for x, y, valid in dl:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        valid = valid.to(device)\n",
        "\n",
        "        logits = model(x)\n",
        "\n",
        "        if valid.sum() == 0:\n",
        "            continue\n",
        "\n",
        "        loss = criterion(logits[valid], y[valid])\n",
        "        iou = compute_iou_from_logits(logits, y, valid).item()\n",
        "        acc = compute_accuracy_from_logits(logits, y, valid).item()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_iou  += iou\n",
        "        total_acc  += acc\n",
        "        n_batches  += 1\n",
        "\n",
        "    if n_batches == 0:\n",
        "        return 0.0, 0.0, 0.0\n",
        "\n",
        "    return (\n",
        "        total_loss / n_batches,\n",
        "        total_iou  / n_batches,\n",
        "        total_acc  / n_batches,\n",
        "    )"
      ],
      "metadata": {
        "id": "AKrxIeKhurr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze encoder → only train decoder/head (classic transfer learning warmup)\n",
        "for p in model.encoder.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()),\n",
        "    lr=1e-3,\n",
        ")\n",
        "\n",
        "print(\"=== Stage 1: Train Decoder Only (Frozen Encoder) ===\")\n",
        "num_epochs_stage1 = 3\n",
        "\n",
        "for epoch in range(num_epochs_stage1):\n",
        "    tr_loss, tr_iou, tr_acc = train_one_epoch(model, train_dl, optimizer)\n",
        "    va_loss, va_iou, va_acc = validate_one_epoch(model, val_dl)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs_stage1}\")\n",
        "    print(f\"  Train - Loss: {tr_loss:.4f}, IoU: {tr_iou:.4f}, Acc: {tr_acc:.4f}\")\n",
        "    print(f\"  Val   - Loss: {va_loss:.4f}, IoU: {va_iou:.4f}, Acc: {va_acc:.4f}\")"
      ],
      "metadata": {
        "id": "ZqJnSK4Iutwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== STAGE 2: Fine-tune Entire Model =====\n",
        "# Unfreeze encoder for better SAR adaptation\n",
        "for p in model.encoder.parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)  # Lower LR\n",
        "\n",
        "print(\"===STAGE 2: Fine-tune Entire Model (Unfrozen Encoder)===\")\n",
        "num_epochs_stage2 = 10\n",
        "\n",
        "for epoch in range(num_epochs_stage2):\n",
        "    tr_loss, tr_iou, tr_acc = train_one_epoch(model, train_dl, optimizer)\n",
        "    va_loss, va_iou, va_acc = validate_one_epoch(model, val_dl)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs_stage2}\")\n",
        "    print(f\"  Train - Loss: {tr_loss:.4f}, IoU: {tr_iou:.4f}, Acc: {tr_acc:.4f}\")\n",
        "    print(f\"  Val   - Loss: {va_loss:.4f}, IoU: {va_iou:.4f}, Acc: {va_acc:.4f}\")"
      ],
      "metadata": {
        "id": "X693PgYRFV84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Before downstream tasks, evaluate segmentation performance:\n",
        "print(\"===Final Test Set Evaluation===\")\n",
        "\n",
        "test_loss, test_iou, test_acc = validate_one_epoch(model, test_dl)\n",
        "print(f\"Test - Loss: {test_loss:.4f}, IoU: {test_iou:.4f}, Acc: {test_acc:.4f}\")"
      ],
      "metadata": {
        "id": "eGk_iQG1GUO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model was trained in two stages following a transfer learning approach.\n",
        "\n",
        "In Stage 1 (frozen encoer), the decoder learned quickly to segment floods, with validation IoU improving from 0.28 to 0.41 over 3 epochs while maintaining a 95% accuracy.\n",
        "\n",
        "In Stage 2 (full fine-tuning), the model continued to improve as the encoder adpated to SAR-specific features, achieving peak validation performance at epoch 9 with an accuracy of 95.8%.\n",
        "\n",
        "The final IoU of 0.45 means the model correctly identifies flood regions with reasonable overlap to ground truth, though there's room for improvement with more data or advanced architectures. The high accuracy reflects the class imbalance (most pixels are non-flood), making IoU the more meaningful metric for flood detection performance."
      ],
      "metadata": {
        "id": "r6HrDhnQSTMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save trained model for later reuse:\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'metrics': {'test_iou': test_iou, 'test_acc': test_acc}\n",
        "}, 'flood_segmentation_model.pth')\n",
        "\n",
        "print(\"Model saved to: flood_segmentation_model.pth\")"
      ],
      "metadata": {
        "id": "FAVLrLlpGXm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the model to reuse and avoid retraining."
      ],
      "metadata": {
        "id": "6gjZRK4aTlb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show example predictions before downstream tasks:\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    x, y, valid = next(iter(test_dl))\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    logits = model(x)\n",
        "    preds = (torch.sigmoid(logits) > 0.5).float()\n",
        "\n",
        "# Visualize first sample\n",
        "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
        "axes[0].imshow(x[0, 0].cpu(), cmap='gray')\n",
        "axes[0].set_title('SAR VV')\n",
        "axes[1].imshow(x[0, 1].cpu(), cmap='gray')\n",
        "axes[1].set_title('SAR VH')\n",
        "axes[2].imshow(y[0, 0].cpu(), cmap='Blues')\n",
        "axes[2].set_title('Ground Truth')\n",
        "axes[3].imshow(preds[0, 0].cpu(), cmap='Blues')\n",
        "axes[3].set_title('Prediction')\n",
        "for ax in axes:\n",
        "    ax.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.savefig('segmentation_results.png', dpi=150)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Et75YXRIGZzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reusable embedding"
      ],
      "metadata": {
        "id": "_TdgLpZJEU31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_embedding(model, x_batch):\n",
        "    \"\"\"\n",
        "    Extracts a reusable embedding from the deepest encoder feature map.\n",
        "    Args:\n",
        "        model: Trained segmentation model\n",
        "        x_batch: (B, 2, H, W) - batch of SAR images\n",
        "\n",
        "    Returns:\n",
        "        pooled: (B, C) - fixed-size embedding per image\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # SMP encoders return a list of feature maps → take deepest one\n",
        "        feat_list = model.encoder(x_batch)      # list of tensors\n",
        "        feats = feat_list[-1]                   # (B, C, H', W')\n",
        "\n",
        "        # Global average pooling over spatial dims\n",
        "        pooled = feats.mean(dim=(2, 3))         # (B, C)\n",
        "    return pooled"
      ],
      "metadata": {
        "id": "OLAwlMjbzi-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We could use it standalone. The `embedding` variable will contain a 512-dimensional feature vector that represents the SAR image in a compressed, learned feature space."
      ],
      "metadata": {
        "id": "2KFOsSeEUyLw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract embedding for a single new SAR image\n",
        "single_image = torch.randn(1, 2, 512, 512).to(device)\n",
        "embedding = extract_embedding(model, single_image)  # (1, 512)\n",
        "\n",
        "print(embedding.shape)\n",
        "print(type(embedding))"
      ],
      "metadata": {
        "id": "svrt98tFUoLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example: does this chip contain any flooded pixels?\n"
      ],
      "metadata": {
        "id": "EPoAxttsEZ8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_embeddings(dataloader, model, device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      Z: (N, C) numpy array of embeddings\n",
        "      Y: (N,) numpy array of chip-level labels (0/1)\n",
        "    \"\"\"\n",
        "    all_z = []\n",
        "    all_y = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for x, y, valid in dataloader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            # 1) Compute embeddings\n",
        "            z = extract_embedding(model, x)  # (B, C)\n",
        "            all_z.append(z.cpu().numpy())\n",
        "\n",
        "            # 2) Create simple chip-level label:\n",
        "            #    1 if any flood pixel exists, else 0\n",
        "            #    (you can refine this, e.g. >1% flood coverage)\n",
        "            y_flat = y.view(y.size(0), -1)\n",
        "            chip_label = (y_flat.max(dim=1).values > 0.5).float()\n",
        "            all_y.append(chip_label.cpu().numpy())\n",
        "\n",
        "    Z = np.concatenate(all_z, axis=0)\n",
        "    Y = np.concatenate(all_y, axis=0)\n",
        "    return Z, Y"
      ],
      "metadata": {
        "id": "hBgKmWt2EeQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Z_train, Y_train = compute_embeddings(train_dl, model, device=device)\n",
        "Z_val,   Y_val   = compute_embeddings(val_dl,   model, device=device)\n",
        "Z_test,  Y_test  = compute_embeddings(test_dl,  model, device=device)\n",
        "\n",
        "print(Z_train.shape, Y_train.shape)"
      ],
      "metadata": {
        "id": "9lHIDJFBHQmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downstream task: chip-level classification\n",
        "\n",
        "We use `Z_train` which is an embedding matrix of shape (47. 512), with each row representing 512-dimensional feature vector produced by the encoder.\n",
        "\n",
        "`Y_train` is the labels, each label = 0 (no flood) or 1 (flood exists somewhere in the chip)"
      ],
      "metadata": {
        "id": "ZfSSmTCLHWYK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DOWNSTREAM TASK 1: Flood Detection Classification"
      ],
      "metadata": {
        "id": "NyZFhRl2V62k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# Train classifier on embeddings\n",
        "clf = LogisticRegression(max_iter=1000, random_state=42)\n",
        "clf.fit(Z_train, Y_train)\n",
        "\n",
        "# Predict on test set\n",
        "Y_pred_test = clf.predict(Z_test)\n",
        "\n",
        "print(f\"\\nTest Set Performance:\")\n",
        "print(f\"  Accuracy: {accuracy_score(Y_test, Y_pred_test):.4f}\")\n",
        "\n",
        "# Only compute F1 if both classes exist in predictions\n",
        "if len(np.unique(Y_pred_test)) > 1:\n",
        "    print(f\"  F1-Score: {f1_score(Y_test, Y_pred_test):.4f}\")\n",
        "else:\n",
        "    print(f\"  F1-Score: N/A (model only predicts one class)\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(Y_test, Y_pred_test,\n",
        "                          target_names=['No Flood', 'Flood'],\n",
        "                          zero_division=0))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(Y_test, Y_pred_test)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['No Flood', 'Flood'],\n",
        "            yticklabels=['No Flood', 'Flood'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix: Flood Classification')\n",
        "plt.tight_layout()\n",
        "plt.savefig('classification_confusion_matrix.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(\"Saved: classification_confusion_matrix.png\")"
      ],
      "metadata": {
        "id": "2VJ4TOyOHYwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###DOWNSTREAM TASK 2: Image Similarity Search"
      ],
      "metadata": {
        "id": "DiLLPxN2YgWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pick a query image from test set\n",
        "query_idx = 0\n",
        "query_chip_id = test_ids[query_idx]\n",
        "\n",
        "# Get its embedding\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    x_query, y_query, valid_query = test_ds[query_idx]\n",
        "    x_query = x_query.unsqueeze(0).to(device)\n",
        "    embedding_query = extract_embedding(model, x_query)\n",
        "\n",
        "# Compute similarity to all training images\n",
        "embedding_query_np = embedding_query.cpu().numpy()\n",
        "similarities = np.dot(Z_train, embedding_query_np.T).flatten()  # Cosine similarity\n",
        "top_k = 5\n",
        "top_indices = similarities.argsort()[-top_k:][::-1]\n",
        "\n",
        "print(f\"\\nQuery: {query_chip_id}\")\n",
        "print(f\"  Flood label: {'Yes' if Y_test[query_idx] == 1 else 'No'}\")\n",
        "print(f\"\\nTop {top_k} most similar training images:\")\n",
        "for rank, idx in enumerate(top_indices, 1):\n",
        "    print(f\"  {rank}. {train_ids[idx]} - Similarity: {similarities[idx]:.4f}, Flood: {'Yes' if Y_train[idx] == 1 else 'No'}\")\n",
        "\n",
        "# Visualize query and top matches\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "\n",
        "# Query image\n",
        "x_q, y_q, _ = test_ds[query_idx]\n",
        "axes[0, 0].imshow(x_q[0].cpu(), cmap='gray')\n",
        "axes[0, 0].set_title(f'Query: {query_chip_id}\\nFlood: {\"Yes\" if Y_test[query_idx] == 1 else \"No\"}')\n",
        "axes[0, 0].axis('off')\n",
        "\n",
        "axes[1, 0].imshow(y_q[0].cpu(), cmap='Blues')\n",
        "axes[1, 0].set_title('Ground Truth')\n",
        "axes[1, 0].axis('off')\n",
        "\n",
        "# Top 2 similar images\n",
        "for i, idx in enumerate(top_indices[:2], 1):\n",
        "    x_similar, y_similar, _ = train_ds[idx]\n",
        "\n",
        "    axes[0, i].imshow(x_similar[0].cpu(), cmap='gray')\n",
        "    axes[0, i].set_title(f'Match {i}: {train_ids[idx]}\\nSim: {similarities[idx]:.3f}')\n",
        "    axes[0, i].axis('off')\n",
        "\n",
        "    axes[1, i].imshow(y_similar[0].cpu(), cmap='Blues')\n",
        "    axes[1, i].set_title(f'Flood: {\"Yes\" if Y_train[idx] == 1 else \"No\"}')\n",
        "    axes[1, i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('similarity_search_results.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(\"Saved: similarity_search_results.png\")"
      ],
      "metadata": {
        "id": "csUW0NISVlRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###DOWNSTREAM TASK 3: Clustering Analysis"
      ],
      "metadata": {
        "id": "erHuQTA9YcW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Cluster embeddings to discover flood patterns\n",
        "n_clusters = 3\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "clusters_train = kmeans.fit_predict(Z_train)\n",
        "clusters_test = kmeans.predict(Z_test)\n",
        "\n",
        "sil_score = silhouette_score(Z_test, clusters_test)\n",
        "print(f\"Silhouette Score: {sil_score:.4f}\")\n",
        "\n",
        "print(f\"\\nCluster distribution (test set):\")\n",
        "for i in range(n_clusters):\n",
        "    cluster_mask = clusters_test == i\n",
        "    n_samples = cluster_mask.sum()\n",
        "    n_floods = Y_test[cluster_mask].sum()\n",
        "    print(f\"  Cluster {i}: {n_samples} samples, {n_floods:.0f} floods ({100*n_floods/n_samples:.1f}%)\")\n",
        "\n",
        "# Visualize clusters\n",
        "fig, axes = plt.subplots(1, n_clusters, figsize=(15, 5))\n",
        "\n",
        "for cluster_id in range(n_clusters):\n",
        "    # Find test samples in this cluster\n",
        "    cluster_mask = clusters_test == cluster_id\n",
        "    cluster_indices = np.where(cluster_mask)[0]\n",
        "\n",
        "    if len(cluster_indices) > 0:\n",
        "        # Show first sample from this cluster\n",
        "        sample_idx = cluster_indices[0]\n",
        "        x_sample, y_sample, _ = test_ds[sample_idx]\n",
        "\n",
        "        axes[cluster_id].imshow(x_sample[0].cpu(), cmap='gray')\n",
        "        axes[cluster_id].set_title(f'Cluster {cluster_id}\\n{cluster_mask.sum()} samples')\n",
        "        axes[cluster_id].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('clustering_examples.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(\"Saved: clustering_examples.png\")"
      ],
      "metadata": {
        "id": "FHh7JqVnVoKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###DOWNSTREAM TASK 4: Compare Two Specific Images"
      ],
      "metadata": {
        "id": "qkZN9edjYVVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare two test images\n",
        "idx_A, idx_B = 0, 1\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    x_A, y_A, _ = test_ds[idx_A]\n",
        "    x_B, y_B, _ = test_ds[idx_B]\n",
        "\n",
        "    x_A_batch = x_A.unsqueeze(0).to(device)\n",
        "    x_B_batch = x_B.unsqueeze(0).to(device)\n",
        "\n",
        "    embedding_A = extract_embedding(model, x_A_batch)\n",
        "    embedding_B = extract_embedding(model, x_B_batch)\n",
        "\n",
        "    similarity = torch.cosine_similarity(embedding_A, embedding_B, dim=1).item()\n",
        "\n",
        "print(f\"\\nComparing two SAR images:\")\n",
        "print(f\"  Image A: {test_ids[idx_A]} - Flood: {'Yes' if Y_test[idx_A] == 1 else 'No'}\")\n",
        "print(f\"  Image B: {test_ids[idx_B]} - Flood: {'Yes' if Y_test[idx_B] == 1 else 'No'}\")\n",
        "print(f\"  Cosine Similarity: {similarity:.4f}\")\n",
        "\n",
        "if similarity > 0.8:\n",
        "    print(\"  → Very similar images (likely same region/conditions)\")\n",
        "elif similarity > 0.5:\n",
        "    print(\"  → Moderately similar images\")\n",
        "else:\n",
        "    print(\"  → Different images (different regions/flood patterns)\")\n",
        "\n",
        "# Visualize comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
        "\n",
        "axes[0, 0].imshow(x_A[0].cpu(), cmap='gray')\n",
        "axes[0, 0].set_title(f'Image A: {test_ids[idx_A]}')\n",
        "axes[0, 0].axis('off')\n",
        "\n",
        "axes[0, 1].imshow(x_B[0].cpu(), cmap='gray')\n",
        "axes[0, 1].set_title(f'Image B: {test_ids[idx_B]}')\n",
        "axes[0, 1].axis('off')\n",
        "\n",
        "axes[1, 0].imshow(y_A[0].cpu(), cmap='Blues')\n",
        "axes[1, 0].set_title(f'Ground Truth A')\n",
        "axes[1, 0].axis('off')\n",
        "\n",
        "axes[1, 1].imshow(y_B[0].cpu(), cmap='Blues')\n",
        "axes[1, 1].set_title(f'Ground Truth B')\n",
        "axes[1, 1].axis('off')\n",
        "\n",
        "fig.suptitle(f'Cosine Similarity: {similarity:.4f}', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig('pairwise_comparison.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(\"Saved: pairwise_comparison.png\")"
      ],
      "metadata": {
        "id": "psJA0zAUVoh5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}