{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "name": "tutorial_v3.ipynb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "173a68e77e1e43eda471464d9562bf0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_76e19247b0c542bbbf1bfc801dfacfd9",
              "IPY_MODEL_7f8dbf6ddc434bcea34105f4528b7dff",
              "IPY_MODEL_01cff07ce6bd442489e9377574dba98e"
            ],
            "layout": "IPY_MODEL_7c1109c8aecb47cc83e07737010e4593"
          }
        },
        "76e19247b0c542bbbf1bfc801dfacfd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a55cf707da8e49baa5eba3d61295b0fb",
            "placeholder": "​",
            "style": "IPY_MODEL_ab372df5f486487dbafb215c2d7404cb",
            "value": "config.json: 100%"
          }
        },
        "7f8dbf6ddc434bcea34105f4528b7dff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14f3595712a14b6f96dcfbbe3c704a7d",
            "max": 156,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1dfb02868ea040baa16897045ae41e6d",
            "value": 156
          }
        },
        "01cff07ce6bd442489e9377574dba98e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a619ee89e73436f885917f97aaa2bd6",
            "placeholder": "​",
            "style": "IPY_MODEL_7430c2b2cb9c4d37bd763b2596289451",
            "value": " 156/156 [00:00&lt;00:00, 16.5kB/s]"
          }
        },
        "7c1109c8aecb47cc83e07737010e4593": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a55cf707da8e49baa5eba3d61295b0fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab372df5f486487dbafb215c2d7404cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "14f3595712a14b6f96dcfbbe3c704a7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1dfb02868ea040baa16897045ae41e6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9a619ee89e73436f885917f97aaa2bd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7430c2b2cb9c4d37bd763b2596289451": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "967b812ce79d4b019f72e71a5435b89c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_27ba91d8bd4e47158bb082fd80ded926",
              "IPY_MODEL_165fca7283954754b79f7e5175fcff3f",
              "IPY_MODEL_1760125c26ac4b749e9cac98f3284101"
            ],
            "layout": "IPY_MODEL_a7fe48d49d91431284afd1efd3172376"
          }
        },
        "27ba91d8bd4e47158bb082fd80ded926": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e89c61cabf9b4300857d7378fbebd59d",
            "placeholder": "​",
            "style": "IPY_MODEL_eeb624fcec3d48b8a56bddfce157c8a2",
            "value": "model.safetensors: 100%"
          }
        },
        "165fca7283954754b79f7e5175fcff3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5fdd556183f493b83ec734f27fa9168",
            "max": 87275112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1a211858658344d0bb1d9af8e1ea58ac",
            "value": 87275112
          }
        },
        "1760125c26ac4b749e9cac98f3284101": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e83df73268524bf7a472ee23f86d3c73",
            "placeholder": "​",
            "style": "IPY_MODEL_e7bf165dc0ac419abdba38b092d873f4",
            "value": " 87.3M/87.3M [00:01&lt;00:00, 55.9MB/s]"
          }
        },
        "a7fe48d49d91431284afd1efd3172376": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e89c61cabf9b4300857d7378fbebd59d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eeb624fcec3d48b8a56bddfce157c8a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d5fdd556183f493b83ec734f27fa9168": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a211858658344d0bb1d9af8e1ea58ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e83df73268524bf7a472ee23f86d3c73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7bf165dc0ac419abdba38b092d873f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hertie-data-science-lab/tutorial-new-group-2-1/blob/xiaohan-modeling/tutorial_v4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial: Transfer Learning for Flood Mapping Using Sentinel-1 Radar Imagery\n",
        "\n",
        "\n",
        "# GRAD-E1394 Deep Learning - Assignment 3\n",
        "\n",
        "Authors:\n",
        "\n",
        "\n",
        "*   Aditi Joshi\n",
        "*   Elena Murray\n",
        "*   Leticia Figueiredo Collado\n",
        "*   Sattiki Ganguly\n",
        "*   Xiaohan Wu\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BPrPCBoKoVny"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test - check commit."
      ],
      "metadata": {
        "id": "seIIHEFETwi7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install segmentation-models-pytorch --quiet\n",
        "!pip install pretrainedmodels --quiet\n",
        "!pip install efficientnet-pytorch --quiet"
      ],
      "metadata": {
        "id": "4GHQh3OEueFH",
        "outputId": "11c3110d-0c4f-4a28-9b92-1aae490f10f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil ls gs://sen1floods11/v1.1/catalog/sen1floods11_hand_labeled_label/ > chip_list.txt\n",
        "\n",
        "with open(\"chip_list.txt\") as f:\n",
        "    chip_dirs = [line.strip() for line in f]\n",
        "\n",
        "chip_dirs = [d for d in chip_dirs if d.endswith(\"_label/\")]\n",
        "\n",
        "def parse_chip(d):\n",
        "    name = d.rstrip(\"/\").split(\"/\")[-1].replace(\"_label\", \"\")\n",
        "    country = name.split(\"_\")[0]\n",
        "    return country, name\n",
        "\n",
        "chip_info = [parse_chip(d) for d in chip_dirs]"
      ],
      "metadata": {
        "id": "N9pPym85uVKX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count chips per country\n",
        "from collections import Counter\n",
        "country_counts = Counter([country for country, _ in chip_info])\n",
        "\n",
        "print(\"Chips per country:\")\n",
        "for country, count in sorted(country_counts.items(), key=lambda x: -x[1])[:10]:\n",
        "    print(f\"  {country}: {count}\")"
      ],
      "metadata": {
        "id": "yHqCyhnj60dl",
        "outputId": "85b94cf8-c5f3-4a59-95e0-b79d8650ce92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chips per country:\n",
            "  USA: 69\n",
            "  India: 68\n",
            "  Paraguay: 67\n",
            "  Ghana: 53\n",
            "  Sri-Lanka: 42\n",
            "  Mekong: 30\n",
            "  Spain: 30\n",
            "  Pakistan: 28\n",
            "  Somalia: 26\n",
            "  Nigeria: 18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_per_country = 50\n",
        "selected_countries = [\"India\", \"USA\", \"Paraguay\"]\n",
        "\n",
        "selected_chips = []\n",
        "for country in selected_countries:\n",
        "    country_chips = [name for c, name in chip_info if c == country]\n",
        "    # Take up to target_per_country from each\n",
        "    selected_chips.extend(country_chips[:target_per_country])\n",
        "\n",
        "print(f\"\\nSelected {len(selected_chips)} chips from {len(selected_countries)} countries\")\n",
        "print(f\"Target: ~150 chips for balanced dataset\")"
      ],
      "metadata": {
        "id": "rq2OK1Um64FV",
        "outputId": "e125a5ba-ea24-4b3c-97e5-1c7ec58d903c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Selected 150 chips from 3 countries\n",
            "Target: ~150 chips for balanced dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import rasterio\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import torchvision.transforms.functional as F\n",
        "import torchvision.transforms as T\n",
        "import random\n",
        "\n",
        "# GCS streaming prefixes\n",
        "HTTP_PREFIX = \"https://storage.googleapis.com/sen1floods11/v1.1\"\n",
        "S1_PREFIX    = f\"/vsicurl/{HTTP_PREFIX}/data/flood_events/HandLabeled/S1Hand\"\n",
        "LABEL_PREFIX = f\"/vsicurl/{HTTP_PREFIX}/data/flood_events/HandLabeled/LabelHand\"\n",
        "\n",
        "class Sentinel1FloodDataset(Dataset):\n",
        "    def __init__(self, id_list, augment = False):\n",
        "        self.ids = id_list\n",
        "        self.augment = augment\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        cid = self.ids[idx]\n",
        "\n",
        "        s1_path    = f\"{S1_PREFIX}/{cid}_S1Hand.tif\"\n",
        "        label_path = f\"{LABEL_PREFIX}/{cid}_LabelHand.tif\"\n",
        "\n",
        "        # --- Load Sentinel-1 SAR image (VV/VH) ---\n",
        "        with rasterio.open(s1_path) as src:\n",
        "            s1_img = src.read().astype(\"float32\")  # (2, 512, 512)\n",
        "\n",
        "        # Robust SAR normalization\n",
        "        s1_img = np.nan_to_num(s1_img)\n",
        "        s1_img = np.clip(s1_img, -50, 50)\n",
        "        s1_img = np.log1p(s1_img - s1_img.min())\n",
        "        s1_img = (s1_img - s1_img.mean()) / (s1_img.std() + 1e-6)\n",
        "\n",
        "        # --- Load flood mask ---\n",
        "        with rasterio.open(label_path) as src:\n",
        "            mask_raw = src.read(1).astype(\"int16\")\n",
        "\n",
        "        valid_mask = (mask_raw != -1)\n",
        "        label = (mask_raw == 1).astype(\"float32\")\n",
        "\n",
        "        # Data augmentation\n",
        "        if self.augment:\n",
        "            if random.random() > 0.5:\n",
        "                s1_img = np.flip(s1_img, axis=2).copy()\n",
        "                label = np.flip(label, axis=1).copy()\n",
        "                valid_mask = np.flip(valid_mask, axis=1).copy()\n",
        "\n",
        "            if random.random() > 0.5:\n",
        "                s1_img = np.flip(s1_img, axis=1).copy()\n",
        "                label = np.flip(label, axis=0).copy()\n",
        "                valid_mask = np.flip(valid_mask, axis=0).copy()\n",
        "\n",
        "            k = random.randint(0, 3)\n",
        "            if k > 0:\n",
        "                s1_img = np.rot90(s1_img, k, axes=(1, 2)).copy()\n",
        "                label = np.rot90(label, k, axes=(0, 1)).copy()\n",
        "                valid_mask = np.rot90(valid_mask, k, axes=(0, 1)).copy()\n",
        "\n",
        "        x = torch.tensor(s1_img, dtype=torch.float32)\n",
        "        y = torch.tensor(label, dtype=torch.float32)[None, ...]\n",
        "        valid = torch.tensor(valid_mask, dtype=torch.bool)[None, ...]\n",
        "\n",
        "        return x, y, valid"
      ],
      "metadata": {
        "id": "JKBD4lzxtZzT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tensor_to_pil_pair(x, y, valid):\n",
        "    vv = x[0].cpu().numpy()\n",
        "    vh = x[1].cpu().numpy()\n",
        "    label_arr = y[0].cpu().numpy()\n",
        "    valid_arr = valid[0].cpu().numpy().astype(np.uint8)\n",
        "\n",
        "    # Normalize to 0-255 range for visualization\n",
        "    vv_norm = ((vv - vv.min()) / (vv.max() - vv.min() + 1e-8) * 255).astype(np.uint8)\n",
        "    vh_norm = ((vh - vh.min()) / (vh.max() - vh.min() + 1e-8) * 255).astype(np.uint8)\n",
        "\n",
        "    vv_pil = Image.fromarray(vv_norm)\n",
        "    vh_pil = Image.fromarray(vh_norm)\n",
        "    label_pil = Image.fromarray((label_arr * 255).astype(np.uint8))\n",
        "    valid_pil = Image.fromarray((valid_arr * 255).astype(np.uint8))\n",
        "\n",
        "    return vv_pil, vh_pil, label_pil, valid_pil"
      ],
      "metadata": {
        "id": "9qdNC7GjthkR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_test(x, y, valid):\n",
        "    return x, y, valid"
      ],
      "metadata": {
        "id": "jDarpVMbt6Nw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_ids = sorted(selected_chips)\n",
        "\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(valid_ids)\n",
        "\n",
        "n = len(valid_ids)\n",
        "train_ids = valid_ids[:int(0.7*n)]\n",
        "val_ids   = valid_ids[int(0.7*n):int(0.85*n)]\n",
        "test_ids  = valid_ids[int(0.85*n):]\n",
        "\n",
        "print(f\"Train: {len(train_ids)}  Val: {len(val_ids)}  Test: {len(test_ids)}\")"
      ],
      "metadata": {
        "id": "nwGhksuguPsV",
        "outputId": "577fe28e-c2a8-4248-acd7-1d1f81efdc93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 105  Val: 22  Test: 23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 4\n",
        "\n",
        "train_ds = Sentinel1FloodDataset(train_ids, augment=True)\n",
        "val_ds   = Sentinel1FloodDataset(val_ids,   augment=False)\n",
        "test_ds  = Sentinel1FloodDataset(test_ids,  augment=False)\n",
        "\n",
        "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_dl   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)\n",
        "test_dl  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "fm52Yv_Nt_1L"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import segmentation_models_pytorch as smp\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# U-Net with ResNet34 encoder pre-trained on ImageNet  ← TRANSFER LEARNING\n",
        "model = smp.Unet(\n",
        "    encoder_name=\"resnet34\",\n",
        "    encoder_weights=\"imagenet\",   # this is the transfer part\n",
        "    in_channels=2,                # VV + VH\n",
        "    classes=1                     # binary mask\n",
        ").to(device)\n",
        "\n",
        "# Binary cross entropy with logits loss\n",
        "# Combines sigmoid activation + BCE loss for numerical stability\n",
        "criterion = nn.BCEWithLogitsLoss()"
      ],
      "metadata": {
        "id": "OpglqQeQuale",
        "outputId": "1b541771-aa37-4f60-cba0-5cd4e7a62a54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203,
          "referenced_widgets": [
            "173a68e77e1e43eda471464d9562bf0a",
            "76e19247b0c542bbbf1bfc801dfacfd9",
            "7f8dbf6ddc434bcea34105f4528b7dff",
            "01cff07ce6bd442489e9377574dba98e",
            "7c1109c8aecb47cc83e07737010e4593",
            "a55cf707da8e49baa5eba3d61295b0fb",
            "ab372df5f486487dbafb215c2d7404cb",
            "14f3595712a14b6f96dcfbbe3c704a7d",
            "1dfb02868ea040baa16897045ae41e6d",
            "9a619ee89e73436f885917f97aaa2bd6",
            "7430c2b2cb9c4d37bd763b2596289451",
            "967b812ce79d4b019f72e71a5435b89c",
            "27ba91d8bd4e47158bb082fd80ded926",
            "165fca7283954754b79f7e5175fcff3f",
            "1760125c26ac4b749e9cac98f3284101",
            "a7fe48d49d91431284afd1efd3172376",
            "e89c61cabf9b4300857d7378fbebd59d",
            "eeb624fcec3d48b8a56bddfce157c8a2",
            "d5fdd556183f493b83ec734f27fa9168",
            "1a211858658344d0bb1d9af8e1ea58ac",
            "e83df73268524bf7a472ee23f86d3c73",
            "e7bf165dc0ac419abdba38b092d873f4"
          ]
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/156 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "173a68e77e1e43eda471464d9562bf0a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/87.3M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "967b812ce79d4b019f72e71a5435b89c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_iou_from_logits(logits, target, valid):\n",
        "    \"\"\"\n",
        "    Compute Intersection over Union (IoU) for flood segmentation.\n",
        "\n",
        "    Args:\n",
        "        logits: (B,1,H,W) - raw model outputs (before sigmoid)\n",
        "        target: (B,1,H,W) - ground truth labels (0/1)\n",
        "        valid:  (B,1,H,W) - validity mask (True for valid pixels)\n",
        "\n",
        "    Returns:\n",
        "        iou: scalar tensor - IoU score\n",
        "    \"\"\"\n",
        "    probs = torch.sigmoid(logits)\n",
        "    preds = (probs > 0.5).float()\n",
        "\n",
        "    v = valid.bool()\n",
        "    if v.sum() == 0:\n",
        "        return torch.tensor(0.0, device=logits.device)\n",
        "\n",
        "    p = preds[v]\n",
        "    t = target[v]\n",
        "\n",
        "    intersection = (p * t).sum()\n",
        "    union = p.sum() + t.sum() - intersection\n",
        "    iou = (intersection + 1e-6) / (union + 1e-6)\n",
        "    return iou\n",
        "\n",
        "\n",
        "def compute_accuracy_from_logits(logits, target, valid):\n",
        "    \"\"\"\n",
        "    Compute pixel-wise accuracy for flood segmentation.\n",
        "\n",
        "    Args:\n",
        "        logits: (B,1,H,W) - raw model outputs (before sigmoid)\n",
        "        target: (B,1,H,W) - ground truth labels (0/1)\n",
        "        valid:  (B,1,H,W) - validity mask (True for valid pixels)\n",
        "\n",
        "    Returns:\n",
        "        acc: scalar tensor - accuracy score\n",
        "    \"\"\"\n",
        "    probs = torch.sigmoid(logits)\n",
        "    preds = (probs > 0.5).float()\n",
        "\n",
        "    v = valid.bool()\n",
        "    if v.sum() == 0:\n",
        "        return torch.tensor(0.0, device=logits.device)\n",
        "\n",
        "    p = preds[v]\n",
        "    t = target[v]\n",
        "\n",
        "    correct = (p == t).float().sum()\n",
        "    acc = correct / p.numel()\n",
        "    return acc"
      ],
      "metadata": {
        "id": "Q1lEehOvuphM"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, dl, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_iou = 0.0\n",
        "    total_acc = 0.0\n",
        "    n_batches = 0\n",
        "\n",
        "    for x, y, valid in dl:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        valid = valid.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(x)  # (B,1,H,W)\n",
        "\n",
        "        if valid.sum() == 0:\n",
        "            continue\n",
        "\n",
        "        loss = criterion(logits[valid], y[valid])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        iou = compute_iou_from_logits(logits, y, valid).item()\n",
        "        acc = compute_accuracy_from_logits(logits, y, valid).item()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_iou  += iou\n",
        "        total_acc  += acc\n",
        "        n_batches  += 1\n",
        "\n",
        "    if n_batches == 0:\n",
        "        return 0.0, 0.0, 0.0\n",
        "\n",
        "    return (\n",
        "        total_loss / n_batches,\n",
        "        total_iou  / n_batches,\n",
        "        total_acc  / n_batches,\n",
        "    )\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate_one_epoch(model, dl):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_iou = 0.0\n",
        "    total_acc = 0.0\n",
        "    n_batches = 0\n",
        "\n",
        "    for x, y, valid in dl:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        valid = valid.to(device)\n",
        "\n",
        "        logits = model(x)\n",
        "\n",
        "        if valid.sum() == 0:\n",
        "            continue\n",
        "\n",
        "        loss = criterion(logits[valid], y[valid])\n",
        "        iou = compute_iou_from_logits(logits, y, valid).item()\n",
        "        acc = compute_accuracy_from_logits(logits, y, valid).item()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_iou  += iou\n",
        "        total_acc  += acc\n",
        "        n_batches  += 1\n",
        "\n",
        "    if n_batches == 0:\n",
        "        return 0.0, 0.0, 0.0\n",
        "\n",
        "    return (\n",
        "        total_loss / n_batches,\n",
        "        total_iou  / n_batches,\n",
        "        total_acc  / n_batches,\n",
        "    )"
      ],
      "metadata": {
        "id": "AKrxIeKhurr_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze encoder → only train decoder/head (classic transfer learning warmup)\n",
        "for p in model.encoder.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()),\n",
        "    lr=1e-3,\n",
        ")\n",
        "\n",
        "print(\"=== Stage 1: Train Decoder Only (Frozen Encoder) ===\")\n",
        "num_epochs_stage1 = 3\n",
        "\n",
        "for epoch in range(num_epochs_stage1):\n",
        "    tr_loss, tr_iou, tr_acc = train_one_epoch(model, train_dl, optimizer)\n",
        "    va_loss, va_iou, va_acc = validate_one_epoch(model, val_dl)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs_stage1}\")\n",
        "    print(f\"  Train - Loss: {tr_loss:.4f}, IoU: {tr_iou:.4f}, Acc: {tr_acc:.4f}\")\n",
        "    print(f\"  Val   - Loss: {va_loss:.4f}, IoU: {va_iou:.4f}, Acc: {va_acc:.4f}\")"
      ],
      "metadata": {
        "id": "ZqJnSK4Iutwa",
        "outputId": "33d12211-cf6a-4e97-8b48-af1ebc91fdc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Stage 1: Train Decoder Only (Frozen Encoder) ===\n",
            "Epoch 1/3\n",
            "  Train - Loss: 0.3170, IoU: 0.2802, Acc: 0.9148\n",
            "  Val   - Loss: 0.1932, IoU: 0.4094, Acc: 0.9401\n",
            "Epoch 2/3\n",
            "  Train - Loss: 0.1909, IoU: 0.2830, Acc: 0.9381\n",
            "  Val   - Loss: 0.1592, IoU: 0.4326, Acc: 0.9538\n",
            "Epoch 3/3\n",
            "  Train - Loss: 0.1873, IoU: 0.3006, Acc: 0.9344\n",
            "  Val   - Loss: 0.1565, IoU: 0.4087, Acc: 0.9527\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== STAGE 2: Fine-tune Entire Model =====\n",
        "# Unfreeze encoder for better SAR adaptation\n",
        "for p in model.encoder.parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)  # Lower LR\n",
        "\n",
        "print(\"===STAGE 2: Fine-tune Entire Model (Unfrozen Encoder)===\")\n",
        "num_epochs_stage2 = 10\n",
        "\n",
        "for epoch in range(num_epochs_stage2):\n",
        "    tr_loss, tr_iou, tr_acc = train_one_epoch(model, train_dl, optimizer)\n",
        "    va_loss, va_iou, va_acc = validate_one_epoch(model, val_dl)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs_stage2}\")\n",
        "    print(f\"  Train - Loss: {tr_loss:.4f}, IoU: {tr_iou:.4f}, Acc: {tr_acc:.4f}\")\n",
        "    print(f\"  Val   - Loss: {va_loss:.4f}, IoU: {va_iou:.4f}, Acc: {va_acc:.4f}\")"
      ],
      "metadata": {
        "id": "X693PgYRFV84",
        "outputId": "eb30233f-c135-494f-98f4-dddf338a1312",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===STAGE 2: Fine-tune Entire Model (Unfrozen Encoder)===\n",
            "Epoch 1/10\n",
            "  Train - Loss: 0.1729, IoU: 0.2842, Acc: 0.9368\n",
            "  Val   - Loss: 0.1415, IoU: 0.4038, Acc: 0.9544\n",
            "Epoch 2/10\n",
            "  Train - Loss: 0.1596, IoU: 0.3407, Acc: 0.9443\n",
            "  Val   - Loss: 0.1399, IoU: 0.4572, Acc: 0.9567\n",
            "Epoch 3/10\n",
            "  Train - Loss: 0.1538, IoU: 0.3629, Acc: 0.9466\n",
            "  Val   - Loss: 0.1809, IoU: 0.2712, Acc: 0.9444\n",
            "Epoch 4/10\n",
            "  Train - Loss: 0.1441, IoU: 0.3725, Acc: 0.9489\n",
            "  Val   - Loss: 0.1420, IoU: 0.4378, Acc: 0.9553\n",
            "Epoch 5/10\n",
            "  Train - Loss: 0.1400, IoU: 0.4219, Acc: 0.9518\n",
            "  Val   - Loss: 0.1472, IoU: 0.4119, Acc: 0.9560\n",
            "Epoch 6/10\n",
            "  Train - Loss: 0.1540, IoU: 0.3759, Acc: 0.9433\n",
            "  Val   - Loss: 0.1614, IoU: 0.3841, Acc: 0.9521\n",
            "Epoch 7/10\n",
            "  Train - Loss: 0.1405, IoU: 0.4006, Acc: 0.9497\n",
            "  Val   - Loss: 0.1617, IoU: 0.3836, Acc: 0.9513\n",
            "Epoch 8/10\n",
            "  Train - Loss: 0.1557, IoU: 0.3927, Acc: 0.9483\n",
            "  Val   - Loss: 0.1629, IoU: 0.3708, Acc: 0.9518\n",
            "Epoch 9/10\n",
            "  Train - Loss: 0.1425, IoU: 0.4055, Acc: 0.9530\n",
            "  Val   - Loss: 0.1672, IoU: 0.3336, Acc: 0.9491\n",
            "Epoch 10/10\n",
            "  Train - Loss: 0.1371, IoU: 0.3746, Acc: 0.9519\n",
            "  Val   - Loss: 0.1258, IoU: 0.4462, Acc: 0.9584\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Before downstream tasks, evaluate segmentation performance:\n",
        "print(\"===Final Test Set Evaluation===\")\n",
        "\n",
        "test_loss, test_iou, test_acc = validate_one_epoch(model, test_dl)\n",
        "print(f\"Test - Loss: {test_loss:.4f}, IoU: {test_iou:.4f}, Acc: {test_acc:.4f}\")"
      ],
      "metadata": {
        "id": "eGk_iQG1GUO3",
        "outputId": "1e05b3d4-3fad-4980-9d25-ab4660c79a7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===Final Test Set Evaluation===\n",
            "Test - Loss: 0.1531, IoU: 0.5290, Acc: 0.9415\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model was trained in two stages following a transfer learning approach.\n",
        "\n",
        "In Stage 1 (frozen encoder), the decoder learned quickly to segment floods, with validation IoU improving from 0.28 to 0.41 over 3 epochs while maintaining a 95% accuracy.\n",
        "\n",
        "In Stage 2 (full fine-tuning), the model continued to improve as the encoder adpated to SAR-specific features, achieving peak validation performance at epoch 9 with an accuracy of 95.8%.\n",
        "\n",
        "The final IoU of 0.45 means the model correctly identifies flood regions with reasonable overlap to ground truth, though there's room for improvement with more data or advanced architectures. The high accuracy reflects the class imbalance (most pixels are non-flood), making IoU the more meaningful metric for flood detection performance."
      ],
      "metadata": {
        "id": "r6HrDhnQSTMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from google.colab import files\n",
        "\n",
        "# Save trained model\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'metrics': {'test_iou': test_iou, 'test_acc': test_acc}\n",
        "}, 'flood_segmentation_model.pth')\n",
        "\n",
        "print(\"✅ Model saved to: flood_segmentation_model.pth\")\n",
        "\n",
        "# Download to your computer\n",
        "print(\"⬇️ Downloading to your computer...\")\n",
        "files.download('flood_segmentation_model.pth')"
      ],
      "metadata": {
        "id": "XTOdOUor2Ixa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# SAVE EMBEDDINGS LOCALLY (Download to Computer)\n",
        "# ========================================\n",
        "\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "\n",
        "# Save embeddings\n",
        "np.savez('sar_embeddings.npz',\n",
        "         Z_train=Z_train, Y_train=Y_train,\n",
        "         Z_val=Z_val, Y_val=Y_val,\n",
        "         Z_test=Z_test, Y_test=Y_test)\n",
        "\n",
        "print(\"✅ Embeddings saved to: sar_embeddings.npz\")\n",
        "\n",
        "# Download to your computer\n",
        "print(\"⬇️ Downloading to your computer...\")\n",
        "files.download('sar_embeddings.npz')\n",
        "\n",
        "print(\"✅ Download complete! Check your Downloads folder.\")"
      ],
      "metadata": {
        "id": "xJGmS26Q1-75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save trained model for later reuse:\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'metrics': {'test_iou': test_iou, 'test_acc': test_acc}\n",
        "}, 'flood_segmentation_model.pth')\n",
        "\n",
        "print(\"Model saved to: flood_segmentation_model.pth\")"
      ],
      "metadata": {
        "id": "FAVLrLlpGXm-",
        "outputId": "c1549fee-0cd9-4590-d15e-15635948f5f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-458011139.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Save trained model for later reuse:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m torch.save({\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;34m'optimizer_state_dict'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m'metrics'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'test_iou'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_iou\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test_acc'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the model to reuse and avoid retraining."
      ],
      "metadata": {
        "id": "6gjZRK4aTlb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show example predictions before downstream tasks:\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    x, y, valid = next(iter(test_dl))\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    logits = model(x)\n",
        "    preds = (torch.sigmoid(logits) > 0.5).float()\n",
        "\n",
        "# Visualize first sample\n",
        "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
        "axes[0].imshow(x[0, 0].cpu(), cmap='gray')\n",
        "axes[0].set_title('SAR VV')\n",
        "axes[1].imshow(x[0, 1].cpu(), cmap='gray')\n",
        "axes[1].set_title('SAR VH')\n",
        "axes[2].imshow(y[0, 0].cpu(), cmap='Blues')\n",
        "axes[2].set_title('Ground Truth')\n",
        "axes[3].imshow(preds[0, 0].cpu(), cmap='Blues')\n",
        "axes[3].set_title('Prediction')\n",
        "for ax in axes:\n",
        "    ax.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.savefig('segmentation_results.png', dpi=150)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Et75YXRIGZzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reusable embedding"
      ],
      "metadata": {
        "id": "_TdgLpZJEU31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_embedding(model, x_batch):\n",
        "    \"\"\"\n",
        "    Extracts a reusable embedding from the deepest encoder feature map.\n",
        "    Args:\n",
        "        model: Trained segmentation model\n",
        "        x_batch: (B, 2, H, W) - batch of SAR images\n",
        "\n",
        "    Returns:\n",
        "        pooled: (B, C) - fixed-size embedding per image\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # SMP encoders return a list of feature maps → take deepest one\n",
        "        feat_list = model.encoder(x_batch)      # list of tensors\n",
        "        feats = feat_list[-1]                   # (B, C, H', W')\n",
        "\n",
        "        # Global average pooling over spatial dims\n",
        "        pooled = feats.mean(dim=(2, 3))         # (B, C)\n",
        "    return pooled"
      ],
      "metadata": {
        "id": "OLAwlMjbzi-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We could use it standalone. The `embedding` variable will contain a 512-dimensional feature vector that represents the SAR image in a compressed, learned feature space."
      ],
      "metadata": {
        "id": "2KFOsSeEUyLw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract embedding for a single new SAR image\n",
        "single_image = torch.randn(1, 2, 512, 512).to(device)\n",
        "embedding = extract_embedding(model, single_image)  # (1, 512)\n",
        "\n",
        "print(embedding.shape)\n",
        "print(type(embedding))"
      ],
      "metadata": {
        "id": "svrt98tFUoLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example: does this chip contain any flooded pixels?\n"
      ],
      "metadata": {
        "id": "EPoAxttsEZ8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_embeddings(dataloader, model, device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      Z: (N, C) numpy array of embeddings\n",
        "      Y: (N,) numpy array of chip-level labels (0/1)\n",
        "    \"\"\"\n",
        "    all_z = []\n",
        "    all_y = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for x, y, valid in dataloader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            # 1) Compute embeddings\n",
        "            z = extract_embedding(model, x)  # (B, C)\n",
        "            all_z.append(z.cpu().numpy())\n",
        "\n",
        "            # 2) Create simple chip-level label:\n",
        "            #    1 if any flood pixel exists, else 0\n",
        "            #    (you can refine this, e.g. >1% flood coverage)\n",
        "            y_flat = y.view(y.size(0), -1)\n",
        "            chip_label = (y_flat.max(dim=1).values > 0.5).float()\n",
        "            all_y.append(chip_label.cpu().numpy())\n",
        "\n",
        "    Z = np.concatenate(all_z, axis=0)\n",
        "    Y = np.concatenate(all_y, axis=0)\n",
        "    return Z, Y"
      ],
      "metadata": {
        "id": "hBgKmWt2EeQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Z_train, Y_train = compute_embeddings(train_dl, model, device=device)\n",
        "Z_val,   Y_val   = compute_embeddings(val_dl,   model, device=device)\n",
        "Z_test,  Y_test  = compute_embeddings(test_dl,  model, device=device)\n",
        "\n",
        "print(Z_train.shape, Y_train.shape)"
      ],
      "metadata": {
        "id": "9lHIDJFBHQmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downstream task: chip-level classification\n",
        "\n",
        "We use `Z_train` which is an embedding matrix of shape (47. 512), with each row representing 512-dimensional feature vector produced by the encoder.\n",
        "\n",
        "`Y_train` is the labels, each label = 0 (no flood) or 1 (flood exists somewhere in the chip)"
      ],
      "metadata": {
        "id": "ZfSSmTCLHWYK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DOWNSTREAM TASK 1: Flood Detection Classification"
      ],
      "metadata": {
        "id": "NyZFhRl2V62k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# Train classifier on embeddings\n",
        "clf = LogisticRegression(max_iter=1000, random_state=42)\n",
        "clf.fit(Z_train, Y_train)\n",
        "\n",
        "# Predict on test set\n",
        "Y_pred_test = clf.predict(Z_test)\n",
        "\n",
        "print(f\"\\nTest Set Performance:\")\n",
        "print(f\"  Accuracy: {accuracy_score(Y_test, Y_pred_test):.4f}\")\n",
        "\n",
        "# Only compute F1 if both classes exist in predictions\n",
        "if len(np.unique(Y_pred_test)) > 1:\n",
        "    print(f\"  F1-Score: {f1_score(Y_test, Y_pred_test):.4f}\")\n",
        "else:\n",
        "    print(f\"  F1-Score: N/A (model only predicts one class)\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(Y_test, Y_pred_test,\n",
        "                          target_names=['No Flood', 'Flood'],\n",
        "                          zero_division=0))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(Y_test, Y_pred_test)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['No Flood', 'Flood'],\n",
        "            yticklabels=['No Flood', 'Flood'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix: Flood Classification')\n",
        "plt.tight_layout()\n",
        "plt.savefig('classification_confusion_matrix.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(\"Saved: classification_confusion_matrix.png\")"
      ],
      "metadata": {
        "id": "2VJ4TOyOHYwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###DOWNSTREAM TASK 2: Image Similarity Search"
      ],
      "metadata": {
        "id": "DiLLPxN2YgWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pick a query image from test set\n",
        "query_idx = 0\n",
        "query_chip_id = test_ids[query_idx]\n",
        "\n",
        "# Get its embedding\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    x_query, y_query, valid_query = test_ds[query_idx]\n",
        "    x_query = x_query.unsqueeze(0).to(device)\n",
        "    embedding_query = extract_embedding(model, x_query)\n",
        "\n",
        "# Compute similarity to all training images\n",
        "embedding_query_np = embedding_query.cpu().numpy()\n",
        "similarities = np.dot(Z_train, embedding_query_np.T).flatten()  # Cosine similarity\n",
        "top_k = 5\n",
        "top_indices = similarities.argsort()[-top_k:][::-1]\n",
        "\n",
        "print(f\"\\nQuery: {query_chip_id}\")\n",
        "print(f\"  Flood label: {'Yes' if Y_test[query_idx] == 1 else 'No'}\")\n",
        "print(f\"\\nTop {top_k} most similar training images:\")\n",
        "for rank, idx in enumerate(top_indices, 1):\n",
        "    print(f\"  {rank}. {train_ids[idx]} - Similarity: {similarities[idx]:.4f}, Flood: {'Yes' if Y_train[idx] == 1 else 'No'}\")\n",
        "\n",
        "# Visualize query and top matches\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "\n",
        "# Query image\n",
        "x_q, y_q, _ = test_ds[query_idx]\n",
        "axes[0, 0].imshow(x_q[0].cpu(), cmap='gray')\n",
        "axes[0, 0].set_title(f'Query: {query_chip_id}\\nFlood: {\"Yes\" if Y_test[query_idx] == 1 else \"No\"}')\n",
        "axes[0, 0].axis('off')\n",
        "\n",
        "axes[1, 0].imshow(y_q[0].cpu(), cmap='Blues')\n",
        "axes[1, 0].set_title('Ground Truth')\n",
        "axes[1, 0].axis('off')\n",
        "\n",
        "# Top 2 similar images\n",
        "for i, idx in enumerate(top_indices[:2], 1):\n",
        "    x_similar, y_similar, _ = train_ds[idx]\n",
        "\n",
        "    axes[0, i].imshow(x_similar[0].cpu(), cmap='gray')\n",
        "    axes[0, i].set_title(f'Match {i}: {train_ids[idx]}\\nSim: {similarities[idx]:.3f}')\n",
        "    axes[0, i].axis('off')\n",
        "\n",
        "    axes[1, i].imshow(y_similar[0].cpu(), cmap='Blues')\n",
        "    axes[1, i].set_title(f'Flood: {\"Yes\" if Y_train[idx] == 1 else \"No\"}')\n",
        "    axes[1, i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('similarity_search_results.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(\"Saved: similarity_search_results.png\")"
      ],
      "metadata": {
        "id": "csUW0NISVlRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###DOWNSTREAM TASK 3: Clustering Analysis"
      ],
      "metadata": {
        "id": "erHuQTA9YcW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Cluster embeddings to discover flood patterns\n",
        "n_clusters = 3\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "clusters_train = kmeans.fit_predict(Z_train)\n",
        "clusters_test = kmeans.predict(Z_test)\n",
        "\n",
        "sil_score = silhouette_score(Z_test, clusters_test)\n",
        "print(f\"Silhouette Score: {sil_score:.4f}\")\n",
        "\n",
        "print(f\"\\nCluster distribution (test set):\")\n",
        "for i in range(n_clusters):\n",
        "    cluster_mask = clusters_test == i\n",
        "    n_samples = cluster_mask.sum()\n",
        "    n_floods = Y_test[cluster_mask].sum()\n",
        "    print(f\"  Cluster {i}: {n_samples} samples, {n_floods:.0f} floods ({100*n_floods/n_samples:.1f}%)\")\n",
        "\n",
        "# Visualize clusters\n",
        "fig, axes = plt.subplots(1, n_clusters, figsize=(15, 5))\n",
        "\n",
        "for cluster_id in range(n_clusters):\n",
        "    # Find test samples in this cluster\n",
        "    cluster_mask = clusters_test == cluster_id\n",
        "    cluster_indices = np.where(cluster_mask)[0]\n",
        "\n",
        "    if len(cluster_indices) > 0:\n",
        "        # Show first sample from this cluster\n",
        "        sample_idx = cluster_indices[0]\n",
        "        x_sample, y_sample, _ = test_ds[sample_idx]\n",
        "\n",
        "        axes[cluster_id].imshow(x_sample[0].cpu(), cmap='gray')\n",
        "        axes[cluster_id].set_title(f'Cluster {cluster_id}\\n{cluster_mask.sum()} samples')\n",
        "        axes[cluster_id].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('clustering_examples.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(\"Saved: clustering_examples.png\")"
      ],
      "metadata": {
        "id": "FHh7JqVnVoKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###DOWNSTREAM TASK 4: Compare Two Specific Images"
      ],
      "metadata": {
        "id": "qkZN9edjYVVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare two test images\n",
        "idx_A, idx_B = 0, 1\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    x_A, y_A, _ = test_ds[idx_A]\n",
        "    x_B, y_B, _ = test_ds[idx_B]\n",
        "\n",
        "    x_A_batch = x_A.unsqueeze(0).to(device)\n",
        "    x_B_batch = x_B.unsqueeze(0).to(device)\n",
        "\n",
        "    embedding_A = extract_embedding(model, x_A_batch)\n",
        "    embedding_B = extract_embedding(model, x_B_batch)\n",
        "\n",
        "    similarity = torch.cosine_similarity(embedding_A, embedding_B, dim=1).item()\n",
        "\n",
        "print(f\"\\nComparing two SAR images:\")\n",
        "print(f\"  Image A: {test_ids[idx_A]} - Flood: {'Yes' if Y_test[idx_A] == 1 else 'No'}\")\n",
        "print(f\"  Image B: {test_ids[idx_B]} - Flood: {'Yes' if Y_test[idx_B] == 1 else 'No'}\")\n",
        "print(f\"  Cosine Similarity: {similarity:.4f}\")\n",
        "\n",
        "if similarity > 0.8:\n",
        "    print(\"  → Very similar images (likely same region/conditions)\")\n",
        "elif similarity > 0.5:\n",
        "    print(\"  → Moderately similar images\")\n",
        "else:\n",
        "    print(\"  → Different images (different regions/flood patterns)\")\n",
        "\n",
        "# Visualize comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
        "\n",
        "axes[0, 0].imshow(x_A[0].cpu(), cmap='gray')\n",
        "axes[0, 0].set_title(f'Image A: {test_ids[idx_A]}')\n",
        "axes[0, 0].axis('off')\n",
        "\n",
        "axes[0, 1].imshow(x_B[0].cpu(), cmap='gray')\n",
        "axes[0, 1].set_title(f'Image B: {test_ids[idx_B]}')\n",
        "axes[0, 1].axis('off')\n",
        "\n",
        "axes[1, 0].imshow(y_A[0].cpu(), cmap='Blues')\n",
        "axes[1, 0].set_title(f'Ground Truth A')\n",
        "axes[1, 0].axis('off')\n",
        "\n",
        "axes[1, 1].imshow(y_B[0].cpu(), cmap='Blues')\n",
        "axes[1, 1].set_title(f'Ground Truth B')\n",
        "axes[1, 1].axis('off')\n",
        "\n",
        "fig.suptitle(f'Cosine Similarity: {similarity:.4f}', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig('pairwise_comparison.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(\"Saved: pairwise_comparison.png\")"
      ],
      "metadata": {
        "id": "psJA0zAUVoh5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}