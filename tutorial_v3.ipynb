{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "name": "tutorial_v3.ipynb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hertie-data-science-lab/tutorial-new-group-2-1/blob/xiaohan-modeling/tutorial_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial: Transfer Learning for Flood Mapping Using Sentinel-1 Radar Imagery\n",
        "\n",
        "\n",
        "# GRAD-E1394 Deep Learning - Assignment 3\n",
        "\n",
        "Authors:\n",
        "\n",
        "\n",
        "*   Aditi Joshi\n",
        "*   Elena Murray\n",
        "*   Leticia Figueiredo Collado\n",
        "*   Sattiki Ganguly\n",
        "*   Xiaohan Wu\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BPrPCBoKoVny"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test - check commit."
      ],
      "metadata": {
        "id": "seIIHEFETwi7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install segmentation-models-pytorch --quiet\n",
        "!pip install pretrainedmodels --quiet\n",
        "!pip install efficientnet-pytorch --quiet"
      ],
      "metadata": {
        "id": "4GHQh3OEueFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil ls gs://sen1floods11/v1.1/catalog/sen1floods11_hand_labeled_label/ > chip_list.txt\n",
        "\n",
        "with open(\"chip_list.txt\") as f:\n",
        "    chip_dirs = [line.strip() for line in f]\n",
        "\n",
        "country_dirs = [d for d in chip_dirs if \"india\" in d.lower()]\n",
        "\n",
        "chip_ids = [d.rstrip(\"/\").split(\"/\")[-1].replace(\"_label\", \"\")\n",
        "            for d in country_dirs]\n",
        "\n",
        "print(f\"Found {len(chip_ids)} India chips\")\n",
        "print(\"Example:\", chip_ids[:5])\n"
      ],
      "metadata": {
        "id": "N9pPym85uVKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import rasterio\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import torchvision.transforms.functional as F\n",
        "import torchvision.transforms as T\n",
        "import random\n",
        "\n",
        "# GCS streaming prefixes\n",
        "HTTP_PREFIX = \"https://storage.googleapis.com/sen1floods11/v1.1\"\n",
        "S1_PREFIX    = f\"/vsicurl/{HTTP_PREFIX}/data/flood_events/HandLabeled/S1Hand\"\n",
        "LABEL_PREFIX = f\"/vsicurl/{HTTP_PREFIX}/data/flood_events/HandLabeled/LabelHand\"\n",
        "\n",
        "class Sentinel1FloodDataset(Dataset):\n",
        "    def __init__(self, id_list):\n",
        "        self.ids = id_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        cid = self.ids[idx]\n",
        "\n",
        "        s1_path    = f\"{S1_PREFIX}/{cid}_S1Hand.tif\"\n",
        "        label_path = f\"{LABEL_PREFIX}/{cid}_LabelHand.tif\"\n",
        "\n",
        "        # --- Load Sentinel-1 SAR image (VV/VH) ---\n",
        "        with rasterio.open(s1_path) as src:\n",
        "            s1_img = src.read().astype(\"float32\")  # (2, 512, 512)\n",
        "\n",
        "        # Robust SAR normalization\n",
        "        s1_img = np.nan_to_num(s1_img)\n",
        "        s1_img = np.clip(s1_img, -50, 50)\n",
        "        s1_img = np.log1p(s1_img - s1_img.min())\n",
        "        s1_img = (s1_img - s1_img.mean()) / (s1_img.std() + 1e-6)\n",
        "\n",
        "        # --- Load flood mask ---\n",
        "        with rasterio.open(label_path) as src:\n",
        "            mask_raw = src.read(1).astype(\"int16\")\n",
        "\n",
        "        valid_mask = (mask_raw != -1)\n",
        "        label = (mask_raw == 1).astype(\"float32\")\n",
        "\n",
        "        x = torch.tensor(s1_img, dtype=torch.float32)\n",
        "        y = torch.tensor(label, dtype=torch.float32)[None, ...]\n",
        "        valid = torch.tensor(valid_mask, dtype=torch.bool)[None, ...]\n",
        "\n",
        "        return x, y, valid"
      ],
      "metadata": {
        "id": "JKBD4lzxtZzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tensor_to_pil_pair(x, y, valid):\n",
        "    vv = x[0].cpu().numpy()\n",
        "    vh = x[1].cpu().numpy()\n",
        "    label_arr = y[0].cpu().numpy()\n",
        "    valid_arr = valid[0].cpu().numpy().astype(np.uint8)\n",
        "\n",
        "    vv_pil = Image.fromarray((vv * 255).astype(np.uint8))\n",
        "    vh_pil = Image.fromarray((vh * 255).astype(np.uint8))\n",
        "    label_pil = Image.fromarray((label_arr * 255).astype(np.uint8))\n",
        "    valid_pil = Image.fromarray((valid_arr * 255).astype(np.uint8))\n",
        "\n",
        "    return vv_pil, vh_pil, label_pil, valid_pil"
      ],
      "metadata": {
        "id": "9qdNC7GjthkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_train(x, y, valid):\n",
        "    vv_pil, vh_pil, label_pil, valid_pil = tensor_to_pil_pair(x, y, valid)\n",
        "\n",
        "    # Random flip (safe)\n",
        "    if random.random() > 0.5:\n",
        "        vv_pil    = F.hflip(vv_pil)\n",
        "        vh_pil    = F.hflip(vh_pil)\n",
        "        label_pil = F.hflip(label_pil)\n",
        "        valid_pil = F.hflip(valid_pil)\n",
        "\n",
        "    if random.random() > 0.5:\n",
        "        vv_pil    = F.vflip(vv_pil)\n",
        "        vh_pil    = F.vflip(vh_pil)\n",
        "        label_pil = F.vflip(label_pil)\n",
        "        valid_pil = F.vflip(valid_pil)\n",
        "\n",
        "    # Convert back to tensors\n",
        "    vv    = F.to_tensor(vv_pil).squeeze(0)\n",
        "    vh    = F.to_tensor(vh_pil).squeeze(0)\n",
        "    label = F.to_tensor(label_pil).round().squeeze(0)\n",
        "    valid = F.to_tensor(valid_pil).round().squeeze(0).bool()\n",
        "\n",
        "    x_aug = torch.stack([vv, vh], dim=0)\n",
        "    y_aug = label.unsqueeze(0)\n",
        "    valid_aug = valid.unsqueeze(0)\n",
        "\n",
        "    return x_aug, y_aug, valid_aug"
      ],
      "metadata": {
        "id": "q1qF5YNit2V6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_test(x, y, valid):\n",
        "    return x, y, valid"
      ],
      "metadata": {
        "id": "jDarpVMbt6Nw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AugmentedSentinel1Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, base_dataset, augment=False):\n",
        "        self.base = base_dataset\n",
        "        self.augment = augment\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, y, valid = self.base[idx]\n",
        "\n",
        "        if self.augment:\n",
        "            return augment_train(x, y, valid)\n",
        "\n",
        "        else:\n",
        "            return preprocess_test(x, y, valid)"
      ],
      "metadata": {
        "id": "2y4RLdTKt-DD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_ids = sorted(chip_ids)\n",
        "\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(valid_ids)\n",
        "\n",
        "n = len(valid_ids)\n",
        "train_ids = valid_ids[:int(0.7*n)]\n",
        "val_ids   = valid_ids[int(0.7*n):int(0.85*n)]\n",
        "test_ids  = valid_ids[int(0.85*n):]\n",
        "\n",
        "print(f\"Train: {len(train_ids)}  Val: {len(val_ids)}  Test: {len(test_ids)}\")"
      ],
      "metadata": {
        "id": "nwGhksuguPsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 4\n",
        "\n",
        "train_ds = AugmentedSentinel1Dataset(Sentinel1FloodDataset(train_ids), augment=True)\n",
        "val_ds   = AugmentedSentinel1Dataset(Sentinel1FloodDataset(val_ids),   augment=False)\n",
        "test_ds  = AugmentedSentinel1Dataset(Sentinel1FloodDataset(test_ids),  augment=False)\n",
        "\n",
        "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "val_dl   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)\n",
        "test_dl  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "fm52Yv_Nt_1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import segmentation_models_pytorch as smp\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# U-Net with ResNet34 encoder pre-trained on ImageNet  ← TRANSFER LEARNING\n",
        "model = smp.Unet(\n",
        "    encoder_name=\"resnet34\",\n",
        "    encoder_weights=\"imagenet\",   # this is the transfer part\n",
        "    in_channels=2,                # VV + VH\n",
        "    classes=1                     # binary mask\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()"
      ],
      "metadata": {
        "id": "OpglqQeQuale"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_iou_from_logits(logits, target, valid):\n",
        "    \"\"\"\n",
        "    logits: (B,1,H,W)\n",
        "    target: (B,1,H,W) with 0/1\n",
        "    valid:  (B,1,H,W) bool\n",
        "    \"\"\"\n",
        "    probs = torch.sigmoid(logits)\n",
        "    preds = (probs > 0.5).float()\n",
        "\n",
        "    v = valid.bool()\n",
        "    if v.sum() == 0:\n",
        "        return torch.tensor(0.0, device=logits.device)\n",
        "\n",
        "    p = preds[v]\n",
        "    t = target[v]\n",
        "\n",
        "    intersection = (p * t).sum()\n",
        "    union = p.sum() + t.sum() - intersection\n",
        "    iou = (intersection + 1e-6) / (union + 1e-6)\n",
        "    return iou\n",
        "\n",
        "\n",
        "def compute_accuracy_from_logits(logits, target, valid):\n",
        "    probs = torch.sigmoid(logits)\n",
        "    preds = (probs > 0.5).float()\n",
        "\n",
        "    v = valid.bool()\n",
        "    if v.sum() == 0:\n",
        "        return torch.tensor(0.0, device=logits.device)\n",
        "\n",
        "    p = preds[v]\n",
        "    t = target[v]\n",
        "\n",
        "    correct = (p == t).float().sum()\n",
        "    acc = correct / p.numel()\n",
        "    return acc"
      ],
      "metadata": {
        "id": "Q1lEehOvuphM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, dl, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_iou = 0.0\n",
        "    total_acc = 0.0\n",
        "    n_batches = 0\n",
        "\n",
        "    for x, y, valid in dl:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        valid = valid.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(x)  # (B,1,H,W)\n",
        "\n",
        "        if valid.sum() == 0:\n",
        "            continue\n",
        "\n",
        "        loss = criterion(logits[valid], y[valid])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        iou = compute_iou_from_logits(logits, y, valid).item()\n",
        "        acc = compute_accuracy_from_logits(logits, y, valid).item()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_iou  += iou\n",
        "        total_acc  += acc\n",
        "        n_batches  += 1\n",
        "\n",
        "    if n_batches == 0:\n",
        "        return 0.0, 0.0, 0.0\n",
        "\n",
        "    return (\n",
        "        total_loss / n_batches,\n",
        "        total_iou  / n_batches,\n",
        "        total_acc  / n_batches,\n",
        "    )\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate_one_epoch(model, dl):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_iou = 0.0\n",
        "    total_acc = 0.0\n",
        "    n_batches = 0\n",
        "\n",
        "    for x, y, valid in dl:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        valid = valid.to(device)\n",
        "\n",
        "        logits = model(x)\n",
        "\n",
        "        if valid.sum() == 0:\n",
        "            continue\n",
        "\n",
        "        loss = criterion(logits[valid], y[valid])\n",
        "        iou = compute_iou_from_logits(logits, y, valid).item()\n",
        "        acc = compute_accuracy_from_logits(logits, y, valid).item()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_iou  += iou\n",
        "        total_acc  += acc\n",
        "        n_batches  += 1\n",
        "\n",
        "    if n_batches == 0:\n",
        "        return 0.0, 0.0, 0.0\n",
        "\n",
        "    return (\n",
        "        total_loss / n_batches,\n",
        "        total_iou  / n_batches,\n",
        "        total_acc  / n_batches,\n",
        "    )"
      ],
      "metadata": {
        "id": "AKrxIeKhurr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze encoder → only train decoder/head (classic transfer learning warmup)\n",
        "for p in model.encoder.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "optimizer = torch.optim.Adam(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()),\n",
        "    lr=1e-3,\n",
        ")\n",
        "\n",
        "print(\"=== Stage 1: Train Decoder Only (Frozen Encoder) ===\")\n",
        "num_epochs_stage1 = 3\n",
        "\n",
        "for epoch in range(num_epochs_stage1):\n",
        "    tr_loss, tr_iou, tr_acc = train_one_epoch(model, train_dl, optimizer)\n",
        "    va_loss, va_iou, va_acc = validate_one_epoch(model, val_dl)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs_stage1}\")\n",
        "    print(f\"  Train - Loss: {tr_loss:.4f}, IoU: {tr_iou:.4f}, Acc: {tr_acc:.4f}\")\n",
        "    print(f\"  Val   - Loss: {va_loss:.4f}, IoU: {va_iou:.4f}, Acc: {va_acc:.4f}\")"
      ],
      "metadata": {
        "id": "ZqJnSK4Iutwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Afrer training, turn the encoder into a reusable embedding"
      ],
      "metadata": {
        "id": "_TdgLpZJEU31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_embedding(model, x_batch):\n",
        "    \"\"\"\n",
        "    Extracts a reusable embedding from the deepest encoder feature map.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # SMP encoders return a list of feature maps → take deepest one\n",
        "        feat_list = model.encoder(x_batch)      # list of tensors\n",
        "        feats = feat_list[-1]                   # (B, C, H', W')\n",
        "\n",
        "        # Global average pooling over spatial dims\n",
        "        pooled = feats.mean(dim=(2, 3))         # (B, C)\n",
        "    return pooled"
      ],
      "metadata": {
        "id": "OLAwlMjbzi-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example: does this chip contain any flooded pixels?\n"
      ],
      "metadata": {
        "id": "EPoAxttsEZ8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_embeddings(dataloader, model, device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      Z: (N, C) numpy array of embeddings\n",
        "      Y: (N,) numpy array of chip-level labels (0/1)\n",
        "    \"\"\"\n",
        "    all_z = []\n",
        "    all_y = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for x, y, valid in dataloader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            # 1) Compute embeddings\n",
        "            z = extract_embedding(model, x)  # (B, C)\n",
        "            all_z.append(z.cpu().numpy())\n",
        "\n",
        "            # 2) Create simple chip-level label:\n",
        "            #    1 if any flood pixel exists, else 0\n",
        "            #    (you can refine this, e.g. >1% flood coverage)\n",
        "            y_flat = y.view(y.size(0), -1)\n",
        "            chip_label = (y_flat.max(dim=1).values > 0.5).float()\n",
        "            all_y.append(chip_label.cpu().numpy())\n",
        "\n",
        "    Z = np.concatenate(all_z, axis=0)\n",
        "    Y = np.concatenate(all_y, axis=0)\n",
        "    return Z, Y"
      ],
      "metadata": {
        "id": "hBgKmWt2EeQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Z_train, Y_train = compute_embeddings(train_dl, model, device=device)\n",
        "Z_val,   Y_val   = compute_embeddings(val_dl,   model, device=device)\n",
        "Z_test,  Y_test  = compute_embeddings(test_dl,  model, device=device)\n",
        "\n",
        "print(Z_train.shape, Y_train.shape)"
      ],
      "metadata": {
        "id": "9lHIDJFBHQmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "47 instead of 48, one batch was skipped during embedding extraction because it contained no valid pixels."
      ],
      "metadata": {
        "id": "BO1X7UXEJOFA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downstream task: chip-level classification\n",
        "\n",
        "We use `Z_train` which is an embedding matrix of shape (47. 512), with each row representing 512-dimensional feature vector produced by the encoder.\n",
        "\n",
        "`Y_train` is the labels, each label = 0 (no flood) or 1 (flood exists somewhere in the chip)"
      ],
      "metadata": {
        "id": "ZfSSmTCLHWYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(Z_train, Y_train)\n",
        "\n",
        "y_pred = clf.predict(Z_test)\n",
        "print(\"Chip-level flood presence accuracy:\", accuracy_score(Y_test, y_pred))\n",
        "print(classification_report(Y_test, y_pred))"
      ],
      "metadata": {
        "id": "2VJ4TOyOHYwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A more complicated downstream task: few-shot flood classification (simulate non-experts with tiny labels)"
      ],
      "metadata": {
        "id": "2JuxA0npKT2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "K = 6  # must be >= 2\n",
        "sss = StratifiedShuffleSplit(n_splits=1, train_size=K)\n",
        "\n",
        "for few_idx, _ in sss.split(Z_train, Y_train):\n",
        "    pass\n",
        "\n",
        "Z_few = Z_train[few_idx]\n",
        "Y_few = Y_train[few_idx]\n",
        "\n",
        "print(\"Few-shot indices:\", few_idx)\n",
        "print(\"Few-shot labels:\", Y_few)\n",
        "\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(Z_few, Y_few)\n",
        "\n",
        "y_pred = clf.predict(Z_test)\n",
        "\n",
        "print(f\"\\nFew-shot ({K}) accuracy:\", accuracy_score(Y_test, y_pred))\n",
        "print(\"\\nClassification report:\\n\", classification_report(Y_test, y_pred))"
      ],
      "metadata": {
        "id": "qbUCX9dUKaOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We should increase the sample size because currently all labels are 1."
      ],
      "metadata": {
        "id": "aNg8p5bwL5gp"
      }
    }
  ]
}